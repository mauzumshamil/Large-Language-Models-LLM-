{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Analysis using LLMs with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by mauzum shamil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Document analysis refers to extracting, interpreting, and understanding the information contained within a document. Traditionally, this involved manual review or simple keyword-based techniques, but with the rise of Large Language Models (LLMs) like GPT and BERT, LLMs are now preferred for document analysis because they can comprehend context, generate summaries, answer questions, and identify key insights efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Text From the pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in document analysis is extracting the content from a PDF file. We can use libraries like pdfplumber to open and read the text from each page of the PDF and save it into a .txt file for further analysis. You can install pdfplumber on your Python environment using the command: pip install pdfplumber. Here’s how to extract text from the PDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " pip install pdfplumber\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text extracted and saved to extracted_text.txt\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "\n",
    "pdf_path = r\"C:\\Users\\mauzu\\OneDrive\\Desktop\\Document Analysis using LLms with Python\\google_terms_of_service_en_in.pdf\"\n",
    "\n",
    "output_text_file = \"extracted_text.txt\"\n",
    "\n",
    "with pdfplumber.open(pdf_path) as pdf:\n",
    "    extracted_text = \"\"\n",
    "    for page in pdf.pages:\n",
    "        extracted_text += page.extract_text()\n",
    "\n",
    "with open(output_text_file, \"w\") as text_file:\n",
    "    text_file.write(extracted_text)\n",
    "\n",
    "print(f\"Text extracted and saved to {output_text_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview the Extracted Text "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After extracting the text, it’s essential to preview the content to ensure everything is correctly captured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GOOGLE TERMS OF SERVICE\n",
      "Effective May 22, 2024 | Archived versions\n",
      "What’s covered in these terms\n",
      "We know it’s tempting to skip these Terms of\n",
      "Service, but it’s important to establish what you\n",
      "can expect from us as you use Google services,\n",
      "and what we expect from you.\n",
      "These Terms of Service re\u0000ect the way Google’s business works, the laws that apply to\n",
      "our company, and certain things we’ve always believed to be true. As a result, these Terms\n",
      "of Service help de\u0000ne Google’s relationship with you as\n"
     ]
    }
   ],
   "source": [
    "# reading pdf content \n",
    "\n",
    "with open(r\"C:\\Users\\mauzu\\OneDrive\\Desktop\\Document Analysis using LLms with Python\\extracted_text.txt\", \"r\") as file:\n",
    "    document_text = file.read()\n",
    "\n",
    "# preview the document content \n",
    "\n",
    "print(document_text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize the Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a high-level overview of the document, you can use a pre-trained summarization model like t5-small. This allows you to condense large pieces of text into shorter summaries, which helps you to grasp the most important information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers -U\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: these Terms of Service reect the way Google’s business works, the laws that apply to our company, and certain things we’ve always believed to be true . these terms include: what you can expect from us, which describes how we provide and develop our services What we expect from you, which establishes certain rules for using our services Content in Google services .\n"
     ]
    }
   ],
   "source": [
    "# load the summarization pipeline\n",
    "summarizer = pipeline(\"summarization\", model=\"t5-small\")\n",
    "\n",
    "# summarize the document text\n",
    "\n",
    "summary = summarizer(document_text[:1000], max_length=150, min_length=30, do_sample=False)\n",
    "print(\"Summary:\", summary[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline(“summarization”, model= “t5-small”) sets up the summarization model using T5-small, a pre-trained transformer model designed for text summarization. The document_text[:1000] specifies the portion of the text to summarize (the first 1000 characters), while max_length = 150 and min_length = 30 control the maximum and minimum length of the summary in tokens. The do_sample = False parameter ensures deterministic output, meaning the model will not randomly sample from possible summaries but will give the same result every time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the Document into Sentences and Passages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more detailed analysis, like question generation, it’s important to split the document into smaller chunks. This step tokenizes the document into sentences and combines them into manageable passages for subsequent steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mauzu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# split text into sentences\n",
    "sentences = sent_tokenize(document_text)\n",
    "\n",
    "# combine sentences into passages\n",
    "passages = []\n",
    "current_passage = \"\"\n",
    "for sentence in sentences:\n",
    "    if len(current_passage.split()) + len(sentence.split()) < 200:  # adjust the word limit as needed\n",
    "        current_passage += \" \" + sentence\n",
    "    else:\n",
    "        passages.append(current_passage.strip())\n",
    "        current_passage = sentence\n",
    "if current_passage:\n",
    "    passages.append(current_passage.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the code, we are using the NLTK library to split the extracted document text into individual sentences using the sent_tokenize() function. Then, we combine these sentences into manageable passages by setting a word limit of 200 words for each passage. This helps ensure that each passage is of a suitable length for further processing by language models, which often have token limits. If the current passage exceeds the word limit, it is appended to the passages list, and the process continues until all sentences are grouped into passages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Questions from the Passages Using LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to generate questions based on the document’s content. This helps in understanding key information points and can be used to check the comprehension of the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tiktoken\n",
    "#pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passage 1:\n",
      "Here are the terms of service that govern how Google operates...\n",
      "\n",
      "Generated Questions:\n",
      "- What are the terms of service that govern how Google operates?\n",
      "- What are the terms and conditions that govern how Google operates?\n",
      "- What are the terms of service for Google?\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Passage 2:\n",
      "The second passage includes detailed descriptions of user responsibilities...\n",
      "\n",
      "Generated Questions:\n",
      "- The second passage includes detailed descriptions of user responsibilities... Provide relevant questions that help understand the text?\n",
      "- The second passage includes detailed descriptions of user responsibilities... Provide relevant questions to help understand the text?\n",
      "- The second passage of the book includes detailed descriptions of user responsibilities... Provide relevant questions that help understand the text?\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, pipeline\n",
    "\n",
    "# Load the T5 tokenizer and model\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"valhalla/t5-base-qg-hl\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"valhalla/t5-base-qg-hl\")\n",
    "\n",
    "# Initialize the pipeline\n",
    "qg_pipeline = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Function to generate questions using the pipeline\n",
    "def generate_questions_pipeline(passage, min_questions=3):\n",
    "    input_text = f\"generate questions for the following text: {passage} Provide relevant questions that help understand the text.\"\n",
    "    results = qg_pipeline(input_text, max_length=512, num_beams=10, num_return_sequences=10)\n",
    "    questions = set()\n",
    "    \n",
    "    for result in results:\n",
    "        questions.update(result['generated_text'].split('<sep>'))\n",
    "    \n",
    "    # Ensure we have at least 3 questions\n",
    "    questions = [q.strip() for q in questions if q.strip() and q.strip().endswith('?')]\n",
    "    \n",
    "    # If fewer than 3 questions, try to regenerate from smaller parts of the passage\n",
    "    if len(questions) < min_questions:\n",
    "        passage_sentences = passage.split('. ')\n",
    "        for i in range(0, len(passage_sentences), 2):\n",
    "            if len(questions) >= min_questions:\n",
    "                break\n",
    "            additional_input = ' '.join(passage_sentences[i:i+2])\n",
    "            additional_results = qg_pipeline(f\"generate questions for the following text: {additional_input} Provide relevant questions that help understand the text.\", max_length=512, num_beams=10, num_return_sequences=10)\n",
    "            for additional_result in additional_results:\n",
    "                questions.update(additional_result['generated_text'].split('<sep>'))\n",
    "            questions = [q.strip() for q in questions if q.strip() and q.strip().endswith('?')]\n",
    "    \n",
    "    return questions[:min_questions]  # Return only the top questions\n",
    "\n",
    "# Generate questions from passages\n",
    "passages = [\"Here are the terms of service that govern how Google operates...\", \"The second passage includes detailed descriptions of user responsibilities...\"]\n",
    "for idx, passage in enumerate(passages):\n",
    "    questions = generate_questions_pipeline(passage)\n",
    "    print(f\"Passage {idx+1}:\\n{passage}\\n\")\n",
    "    print(\"Generated Questions:\")\n",
    "    for q in questions:\n",
    "        print(f\"- {q}\")\n",
    "    print(f\"\\n{'-'*50}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the code, we are using a question generation model (T5-based model valhalla/t5-base-qg-hl) from the Hugging Face transformers library to automatically generate questions from text passages. The function generate_questions_pipeline() takes a text passage as input and produces a list of questions. We generate at least three questions for each passage, and if not, we split the passage into smaller parts and generate additional questions. This approach guarantees comprehensive question generation for each passage, and we print the questions along with the corresponding passage for review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer the Generated Questions Using a QA Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After generating the questions, we can use a pre-trained question-answering (QA) model to find the answers within the text. The deepset/roberta-base-squad2 model extracts answers based on the context of the passage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "c:\\Users\\mauzu\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\question_answering.py:391: FutureWarning: Passing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What are the terms of service that govern how Google operates?\n",
      "A: Here\n",
      "\n",
      "Q: What are the terms and conditions that govern how Google operates?\n",
      "A: terms of service\n",
      "\n",
      "Q: What are the terms of service for Google?\n",
      "A: terms of service that govern how Google operates...\n",
      "\n",
      "==================================================\n",
      "\n",
      "Q: The second passage includes detailed descriptions of user responsibilities... Provide relevant questions that help understand the text?\n",
      "A: ...\n",
      "\n",
      "Q: The second passage includes detailed descriptions of user responsibilities... Provide relevant questions to help understand the text?\n",
      "A: ...\n",
      "\n",
      "Q: The second passage of the book includes detailed descriptions of user responsibilities... Provide relevant questions that help understand the text?\n",
      "A: ...\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the QA pipeline with additional parameters for detailed answers\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\", \n",
    "                       max_answer_len=512, num_beams=5)\n",
    "\n",
    "# Function to track and answer only unique questions\n",
    "def answer_unique_questions(passages, qa_pipeline):\n",
    "    answered_questions = set()  # to store unique questions\n",
    "\n",
    "    for idx, passage in enumerate(passages):\n",
    "        questions = generate_questions_pipeline(passage)\n",
    "\n",
    "        for question in questions:\n",
    "            if question not in answered_questions:  # check if the question has already been answered\n",
    "                answer = qa_pipeline({'question': question, 'context': passage})\n",
    "                print(f\"Q: {question}\")\n",
    "                print(f\"A: {answer['answer']}\\n\")\n",
    "                answered_questions.add(question)  # add the question to the set to avoid repetition\n",
    "        print(f\"{'='*50}\\n\")\n",
    "\n",
    "# Example passages\n",
    "passages = [\n",
    "    \"Here are the terms of service that govern how Google operates...\",\n",
    "    \"The second passage includes detailed descriptions of user responsibilities...\"\n",
    "]\n",
    "\n",
    "answer_unique_questions(passages, qa_pipeline)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the code, we used a question-answering (QA) pipeline with the deepset/roberta-base-squad2 model to answer questions generated from the document passages. The function answer_unique_questions() tracks unique questions in a set to ensure it answers each question only once. As the code processes each passage, it checks whether it has already answered a question; if not, it generates an answer based on the passage’s context. This avoids answering duplicate questions and ensures efficient processing of all relevant queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, this is how we can analyze documents using LLMs step-by-step. LLMs excel at understanding natural language, which makes them ideal for handling complex documents and extracting meaningful insights with high accuracy and minimal human intervention. I hope you liked this article on document analysis using LLMs with Python. Feel free to ask valuable questions in the comments section below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are my profiles for reference:\n",
    "\n",
    "GitHub:\n",
    "https://github.com/mauzumshamil\n",
    "\n",
    "LinkedIn:\n",
    "http://linkedin.com/in/mauzum-shamil\n",
    "\n",
    "Portfolio Link: \n",
    "https://linktr.ee/mauzum_shamil"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
